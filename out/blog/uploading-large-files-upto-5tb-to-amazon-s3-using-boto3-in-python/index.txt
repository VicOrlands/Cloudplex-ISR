3:I[4707,[],""]
5:I[36423,[],""]
6:I[83575,["1779","static/chunks/0e762574-e2250de400a4ed4b.js","5452","static/chunks/5e22fd23-548660fd7966a330.js","5878","static/chunks/5878-5874fb2c0dc2a1fa.js","2972","static/chunks/2972-ec2c5667a187e8c4.js","6434","static/chunks/6434-f1ba83ef6f4d8ff1.js","3464","static/chunks/3464-108d3965859ff6f4.js","7261","static/chunks/7261-55c1cedd72bd6eaa.js","8003","static/chunks/8003-22382164f812e9a5.js","3185","static/chunks/app/layout-07248a66e26d07dc.js"],"default"]
7:I[72972,["5878","static/chunks/5878-5874fb2c0dc2a1fa.js","2972","static/chunks/2972-ec2c5667a187e8c4.js","6434","static/chunks/6434-f1ba83ef6f4d8ff1.js","5915","static/chunks/5915-0201d1e543d87633.js","3486","static/chunks/3486-31c46a73f5eb8de6.js","273","static/chunks/273-4de1aefaf8cd17ce.js","308","static/chunks/app/blog/%5Bslug%5D/page-e7b2d01086a187e4.js"],""]
8:"$Sreact.suspense"
9:I[81523,["6051","static/chunks/795d4814-476d6416c8e2052d.js","1779","static/chunks/0e762574-e2250de400a4ed4b.js","5878","static/chunks/5878-5874fb2c0dc2a1fa.js","2972","static/chunks/2972-ec2c5667a187e8c4.js","3705","static/chunks/3705-b8fe6f5208fe399e.js","3978","static/chunks/3978-b267f049d115c2b8.js","1110","static/chunks/1110-de266bf53394e4db.js","1055","static/chunks/1055-c276cbff28cfad9a.js","4606","static/chunks/4606-20a0c42850a47167.js","2403","static/chunks/2403-4383a29793ac9144.js","3839","static/chunks/3839-050ccec6cac83fe9.js","1931","static/chunks/app/page-c8486484b9037db3.js"],"BailoutToCSR"]
a:I[7237,["1779","static/chunks/0e762574-e2250de400a4ed4b.js","5452","static/chunks/5e22fd23-548660fd7966a330.js","5878","static/chunks/5878-5874fb2c0dc2a1fa.js","2972","static/chunks/2972-ec2c5667a187e8c4.js","6434","static/chunks/6434-f1ba83ef6f4d8ff1.js","3464","static/chunks/3464-108d3965859ff6f4.js","7261","static/chunks/7261-55c1cedd72bd6eaa.js","8003","static/chunks/8003-22382164f812e9a5.js","3185","static/chunks/app/layout-07248a66e26d07dc.js"],"default"]
b:I[22207,["1779","static/chunks/0e762574-e2250de400a4ed4b.js","5452","static/chunks/5e22fd23-548660fd7966a330.js","5878","static/chunks/5878-5874fb2c0dc2a1fa.js","2972","static/chunks/2972-ec2c5667a187e8c4.js","6434","static/chunks/6434-f1ba83ef6f4d8ff1.js","3464","static/chunks/3464-108d3965859ff6f4.js","7261","static/chunks/7261-55c1cedd72bd6eaa.js","8003","static/chunks/8003-22382164f812e9a5.js","3185","static/chunks/app/layout-07248a66e26d07dc.js"],"default"]
c:I[75549,["1779","static/chunks/0e762574-e2250de400a4ed4b.js","5452","static/chunks/5e22fd23-548660fd7966a330.js","5878","static/chunks/5878-5874fb2c0dc2a1fa.js","2972","static/chunks/2972-ec2c5667a187e8c4.js","6434","static/chunks/6434-f1ba83ef6f4d8ff1.js","3464","static/chunks/3464-108d3965859ff6f4.js","7261","static/chunks/7261-55c1cedd72bd6eaa.js","8003","static/chunks/8003-22382164f812e9a5.js","3185","static/chunks/app/layout-07248a66e26d07dc.js"],"default"]
4:["slug","uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python","d"]
0:["uEu_oHx93E1n1djLkNdZ9",[[["",{"children":["blog",{"children":[["slug","uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python","d"],{"children":["__PAGE__?{\"slug\":\"uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["blog",{"children":[["slug","uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python","d"],{"children":["__PAGE__",{},[["$L1","$L2",[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/31e337c2aa3be57f.css","precedence":"next","crossOrigin":"$undefined"}]]],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/01329c7d2530a3fe.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/a2429df6874052f9.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","className":"__variable_d65c78","children":["$","body",null,{"children":[["$","$L6",null,{}],["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"style":{"display":"flex","flexDirection":"column","justifyContent":"center","alignItems":"center","height":"100vh","textAlign":"center","fontFamily":"Arial, sans-serif","backgroundColor":"#f9f9f9","color":"#333","padding":"0 20px"},"children":[["$","div",null,{"style":{"marginBottom":"20px","color":"#555"},"children":["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","width":"150","height":"150","fill":"none","stroke":"currentColor","strokeWidth":"2","strokeLinecap":"round","strokeLinejoin":"round","children":[["$","circle",null,{"cx":"12","cy":"12","r":"10"}],["$","line",null,{"x1":"15","y1":"9","x2":"9","y2":"15"}],["$","line",null,{"x1":"9","y1":"9","x2":"15","y2":"15"}]]}]}],["$","h1",null,{"style":{"fontSize":"3rem","marginBottom":"10px"},"children":"404 - Page Not Found"}],["$","p",null,{"style":{"fontSize":"1.6rem","marginBottom":"20px"},"children":"Oops! The page you are looking for does not exist."}],["$","$L7",null,{"href":"/","style":{"fontSize":"1.4rem","color":"#0070f3","textDecoration":"none"},"children":"Return to Home"}]]}],"notFoundStyles":[]}],["$","$8",null,{"fallback":null,"children":["$","$L9",null,{"reason":"next/dynamic","children":["$","$La",null,{}]}]}],[["$","$Lb",null,{}],["$","$Lc",null,{}]]]}]}]],null],null],["$Ld",null]]]]
e:I[34027,["5878","static/chunks/5878-5874fb2c0dc2a1fa.js","2972","static/chunks/2972-ec2c5667a187e8c4.js","6434","static/chunks/6434-f1ba83ef6f4d8ff1.js","5915","static/chunks/5915-0201d1e543d87633.js","3486","static/chunks/3486-31c46a73f5eb8de6.js","273","static/chunks/273-4de1aefaf8cd17ce.js","308","static/chunks/app/blog/%5Bslug%5D/page-e7b2d01086a187e4.js"],"default"]
10:I[49747,["5878","static/chunks/5878-5874fb2c0dc2a1fa.js","2972","static/chunks/2972-ec2c5667a187e8c4.js","6434","static/chunks/6434-f1ba83ef6f4d8ff1.js","5915","static/chunks/5915-0201d1e543d87633.js","3486","static/chunks/3486-31c46a73f5eb8de6.js","273","static/chunks/273-4de1aefaf8cd17ce.js","308","static/chunks/app/blog/%5Bslug%5D/page-e7b2d01086a187e4.js"],"default"]
f:T1c80,Amazon Simple Storage Service (S3) is a widely-used cloud storage service that allows users to store and retrieve any amount of data at any time. Uploading large files, especially those approaching the terabyte scale, can be challenging. Boto3, the AWS SDK for Python, provides a powerful and flexible way to interact with S3, including handling large file uploads through its multipart upload feature.

## Prerequisites

Before we begin, make sure you have the following:

1. **AWS Account**: You need an AWS account with appropriate permissions to access S3.

2. **Boto3 Installation**: Install Boto3 by running `pip install boto3` in your terminal.

3. **AWS Credentials**: Set up your AWS credentials, either by configuring the AWS CLI (`aws configure`) or directly within your script.

## Why Multipart?

- While single-file uploads using **Presigned URLs** are limited to a maximum of **5GB**, multipart uploads can handle files up to **5TB**.

- Multipart uploads are efficient for large files, especially when parallelization can be leveraged to speed up the process.

- Presigned URLs may introduce additional latency, as each part of the file requires a separate HTTP request.

### Benefits

S3 Multipart Upload is beneficial for handling large files efficiently. Here are key reasons to use it:

1. **Efficiency for Large Files:**
   
   - Splits large files into smaller parts for better handling.

2. **Resilience to Failures:**
   
   - Reduces the risk of failure by allowing resumption from the point of interruption.

3. **Parallel Uploads:**
   
   - Speeds up uploads by enabling parallel uploading of file parts.

4. **Optimal for Unstable Connections:**
   
   - Minimizes the impact of network failures by retrying only the failed parts.

5. **Support for Transfer Acceleration:**
   
   - Compatible with S3 Transfer Acceleration for faster uploads.

6. **SDK Support:**
   
   - AWS SDKs offer built-in support, simplifying implementation.

7. **Concurrency Control:**
   
   - Allows control over the number of parallel uploads.

## Writing the Python Script

Let's create a Python script that utilizes Boto3 to upload a large file to S3 in a multipart fashion. 

```python
import boto3
from boto3.s3.transfer import TransferConfig

# Set your AWS credentials and region
aws_access_key_id = 'YOUR_ACCESS_KEY_ID'
aws_secret_access_key = 'YOUR_SECRET_ACCESS_KEY'
region_name = 'YOUR_REGION'

# Set your S3 bucket and object key
bucket_name = 'YOUR_BUCKET_NAME'
object_key = 'your-prefix/your-large-file.tar.gz'

# Specify the local file to upload
local_file_path = 'path/to/your-large-file.tar.gz'

# Set the desired part size and number of threads
part_size_mb = 50  # You can adjust this based on your requirements
num_threads = 10

# Create an S3 client
s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)

# Create a TransferConfig object
transfer_config = TransferConfig(multipart_threshold=part_size_mb * 1024 * 1024, max_concurrency=num_threads)

# Create an S3 transfer manager
transfer_manager = boto3.s3.transfer.TransferManager(s3, config=transfer_config)

try:
    # Upload the file using multipart upload
    upload = transfer_manager.upload(local_file_path, bucket_name, object_key)

    # Wait for the upload to complete
    upload.wait()

    print(f"File uploaded successfully to {bucket_name}/{object_key}")

except Exception as e:
    print(f"Error uploading file: {e}")

finally:
    # Clean up resources
    transfer_manager.shutdown()
```

## Understanding the Script

Let's break down the key components of the script:

1. **AWS Credentials and Configuration**: Set your AWS credentials (access key and secret key) and the AWS region where your S3 bucket is located.
   
   ```python
   # Set your AWS credentials and region
   aws_access_key_id = 'YOUR_ACCESS_KEY_ID'
   aws_secret_access_key = 'YOUR_SECRET_ACCESS_KEY'
   region_name = 'YOUR_REGION'
   ```

2. **S3 Bucket and Object Key**: Define the target S3 bucket and the object key (path) under which the file will be stored.
   
   ```python
   # Set your S3 bucket and object key
   bucket_name = 'YOUR_BUCKET_NAME'
   object_key = 'your-prefix/your-large-file.tar.gz'
   ```

3. **Local File Path**: Specify the local path of the large file you want to upload.
   
   ```python
   # Specify the local file to upload
   local_file_path = 'path/to/your-large-file.tar.gz'
   ```

4. **Part Size and Concurrency**: Determine the part size in megabytes and the number of threads to use during the multipart upload. Adjust these values based on your network conditions and requirements.
   
   ```python
   # Set the desired part size and number of threads
   part_size_mb = 50  # You can adjust this based on your requirements
   num_threads = 10
   ```

5. **Creating S3 Client and Transfer Manager**: Initialize the Boto3 S3 client and create a TransferConfig object with the specified multipart settings.
   
   ```python
   # Create an S3 client
   s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)
   ```

6. **Multipart Upload**: Use the TransferManager to initiate a multipart upload of the specified file to the S3 bucket. This method automatically handles the division of the file into parts and manages the upload process.
   
   ```python
   # Create a TransferConfig object
   transfer_config = TransferConfig(multipart_threshold=part_size_mb * 1024 * 1024, max_concurrency=num_threads)
   ```

7. **Wait for Upload Completion**: Wait for the multipart upload to complete before proceeding. This ensures that all parts are successfully uploaded and assembled on the S3 bucket.
   
   ```python
   # Wait for the upload to complete
   upload.wait()
   ```

8. **Clean Up**: Finally, clean up resources by shutting down the TransferManager.
   
   ```python
   # Clean up resources
   transfer_manager.shutdown()
   ```

## Running the Script

To run the script:

1. Save the script to a file (e.g., `upload_to_s3.py`).
2. Open a terminal and navigate to the script's directory.
3. Run the script using the command `python upload_to_s3.py`.

Ensure that the AWS credentials have the necessary permissions to perform S3 uploads.

## Conclusion

Uploading large files to Amazon S3 using Boto3 in Python becomes a manageable task with the multipart upload feature. By breaking down the file into smaller parts and uploading them concurrently, you can efficiently transfer large datasets to S3. Adjusting parameters such as part size and concurrency allows you to optimize the upload process based on your specific requirements. Incorporating this approach into your workflow facilitates the seamless transfer of large files to the cloud, unlocking the full potential of Amazon S3 for scalable and reliable storage.



**Continue Reading**

[Top Cloud Services providers in Nigeria with CloudPlexo's Innovative Solutions](https://cloudplexo.com)

[Understanding the Difference Between AWS SNS and SQS](https://cloudplexo.com/understanding-the-difference-between-aws-sns-and-sqs)

[Uploading and Downloading Files to/from Amazon S3 using Boto3](https://cloudplexo.com/uploading-and-downloading-files-from-amazon-sw3-using-boto3)2:["$","$Le",null,{"blog":{"content":"$f","date":"2024-01-31","published":false,"updated":"12/2/2024","slug":"uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python","description":"Amazon Simple Storage Service (S3) is a widely-used cloud storage service that allows users to store and retrieve any amount of data at any time. Uploading large files, especially those approaching the terabyte scale, can be challenging. Boto3, the AWS SDK for Python, provides a powerful and flexible way to interact with S3, including handling large file uploads through its multipart upload feature.\n","key":"uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python","thumbnail":"https://cloudplexo-cms-bucket.s3.af-south-1.amazonaws.com/blog/uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python/1733104504157-uploading-large-files-to-s3-thumbnail.png","author":"abdulmumin yaqeen","title":"Uploading Large Files Upto 5TB to Amazon S3 using Boto3 in Python"},"oldBlog":{"comp":["$","$L10",null,{}],"url":"uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python"}}]
d:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Uploading Large Files Upto 5TB to Amazon S3 using Boto3 in Python - CloudPlexo Blog"}],["$","meta","3",{"name":"description","content":"Uploading large files, especially those approaching the terabyte scale, can be challenging. Boto3, the AWS SDK for Python, provides a powerful and flexible way to interact with S3, including handling large file uploads through its multipart upload feature."}],["$","meta","4",{"name":"keywords","content":"uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python,Cloud Computing Insights,AWS Blog,Azure Articles,Google Cloud Updates,Cloud Technology Trends"}],["$","link","5",{"rel":"canonical","href":"https://www.cloudplexo.com/blog/uploading-large-files-upto-5tb-to-amazon-s3-using-boto3-in-python/"}],["$","link","6",{"rel":"icon","href":"/icon.ico?7156ce2ac441bdbe","type":"image/x-icon","sizes":"16x16"}],["$","meta","7",{"name":"next-size-adjust"}]]
1:null
